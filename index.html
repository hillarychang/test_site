<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Cash Score: Alternative Credit Scoring</title>
        <meta name="description"
            content="2024 UC San Diego Data Science Capstone Project, written by Section B18 Group X.">
        <meta name="author" content="Jevan Chahal, Hillary Chang, Kurumi Kaneko, Kevin Wong">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Tilt+Warp&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:ital,wght@0,500;1,500&display=swap" rel="stylesheet">
        <link type="text/css" href="style.css" rel="stylesheet">
        <link href="media/ucsd-icon.png" rel="icon">
    </head>
    <body>
        <header>
            <h1>Cash Score: Alternative Credit Scoring</h1>
            <h2>Using Banking Transaction Data for Fairer Credit Assessment</h2>
            <div class="authors">
                <p>By:
                    <a href="mailto:j2chahal@ucsd.edu">Jevan Chahal</a>,
                    <a href="mailto:hic001@ucsd.edu">Hillary Chang</a>,
                    <a href="mailto:kskaneko@ucsd.edu">Kurumi Kaneko</a>, and
                    <a href="mailto:kew024@ucsd.edu">Kevin Wong</a>.
                </p>
                <p>Mentors:
                    <a href="mailto:brian.duke@prismdata.com">Brian Duke (Prism Data, Inc.)</a> and
                    <a href="mailto:kyle.nero@prismdata.com">Kyle Nero (Prism Data, Inc.)</a>.
                </p>
            </div>
            <div class="links">
                <a href="https://github.com/hillarychang/dsc180b-capstone-q2"
                    class="resource-links" target="_blank" rel="noopener noreferrer">View Code</a>
                <a href="document/poster.pdf" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Poster</a>
                <a href="document/report.pdf" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Report</a>
            </div>
            <nav>
                <ul>
                    <li><a href="#abstract">Abstract</a></li>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#methods">Methods</a></li>
                    <li><a href="#results">Results</a></li>
                    <li><a href="#model-output">Model Output</a></li>
                    <li><a href="#discussion">Discussion</a></li>
                    <li><a href="#contribution">Contributions</a></li>
                    <li><a href="#contact">Contacts</a></li>
                </ul>
            </nav>
        </header>
        <hr />
        <main>
            <section id="abstract">
                <h3>Abstract</h3>
                <p>The process of determining a creditor's trustworthiness is crucial within the context of bank data, given the ethical and regulatory constraints surrounding its use. Despite the vast quantity of available data, only a limited number of features are explicitly useful for machine learning applications, raising the question of how best to assess a customer's financial reliability. Our methodology involves refining bank data into meaningful categories using Natural Language Processing, estimating individual income based on transaction data alone, and evaluating creditworthiness with both accuracy and efficiency.</p>
            </section>
            
            <section id="introduction">
                <h3>Introduction</h3>
                <p>Determining a customer’s creditworthiness is essential for banks, influencing decisions about loans, credit cards, and other financial services. While traditional credit scores are widely used, they often fail to capture the full financial picture, especially for individuals with little to no credit history. Our project aims to bridge this gap by leveraging transaction data to assess financial behavior rather than relying solely on past credit activity. By analyzing spending habits, income consistency, and transaction types, we seek to create a scoring model that provides a more accurate and equitable reflection of financial reliability. As part of our project, we have implemented a data pipeline to support this model, including a categorization system that classifies transaction memos from vendors with high accuracy and low latency—ensuring that entries like "Amazon.com" are correctly categorized under "General Merchandise." This categorization plays a key role in our broader goal of developing the Cash Score, a numerical value ranging from 1 to 999 that predicts a consumer’s likelihood of defaulting on debt or credit. By equipping financial institutions with this tool, we aim to enhance risk assessment while ensuring that credit access is extended to those with strong banking histories, making lending practices more inclusive and effective.</p>
            </section>
            
            <section id="methods">
                <h3>Methods</h3>
                <p>Our methodology consists of feature engineering from transaction data, model training using various machine learning algorithms, and performance evaluation based on key metrics. The figure below shows our overall methodology:</p>
    
                <figure>
                    <img src="figure/methodology.png" alt="Overall Methodology Diagram">
                    <figcaption>Figure: Overview of our methodology.</figcaption>
                </figure>
            </section>

<!--        <p>Our project is structured around two primary objectives: categorizing transactions based on memos and developing the <strong>Cash Score</strong>, a predictive model for assessing a consumer’s likelihood of defaulting on credit.</p> -->

            <!-- Categorizing Transactions based on Memos -->
            <section id="categorizing-transactions">
                <h3>Categorizing Transactions Based on Memos</h3>

                <!-- Feature Creation and Selection -->
                <section id="feature-creation">

                    <h4><strong>Feature Creation and Selection</strong></h4>
            
                    <!-- Token Augmentation -->
                    <details>
                        <summary>Token Augmentation</summary>
                    
                        <p>
                            To prepare the transaction data for further analysis and modeling, we enhanced the <code>memo</code> field by adding specific tokens based on transaction characteristics. 
                            This token augmentation adds additional context about transaction amounts and dates, helping the model learn more meaningful patterns.
                        </p>
                    
                        <div class="image-container">
                            <figure>
                                <img src="figure/pre_token.jpeg" alt="Pre-token augmentation transaction data">
                                <figcaption>Figure: This is the transaction data cleaned but not token augmented yet. The <code>memo</code> field contains raw text descriptions without any added tokens.</figcaption>
                            </figure>
                        </div>
                    
                        <p>The following steps outline the transformation process:</p>
                    
                        <ul>
                            <li>
                                <strong>Whole Dollar Amount Identification:</strong> We added a token <code>&lt;W_D&gt;</code> to the <code>memo</code> field for transactions with whole dollar amounts.
                                This token helps identify transactions that might involve cash withdrawals or ATM transactions, as these are typically in round amounts (e.g., $20, $50).
                            </li>
            
                            <li>
                                <strong>Day of Transaction:</strong> For each transaction's posting date, we generated a day token in the format <code>&lt;D_day&gt;</code>, where <code>day</code> represents the specific 
                                day of the month. This token helps analyze patterns such as end-of-month or beginning-of-month spending habits.
                            </li>
            
                            <li>
                                <strong>Month of Transaction:</strong> Similarly, we generated a month token in the format <code>&lt;M_month&gt;</code> (where <code>month</code> is the numerical month) 
                                to capture seasonal or monthly spending trends.
                            </li>
            
                            <li>
                                <strong>Token Augmentation Process:</strong> For each row in the dataset, these tokens were concatenated to the <code>memo</code> field using the following transformation:
                                <pre>
            memo = memo + whole_dollar_amount_token + day_token + month_token
                                </pre>
                            </li>
                        </ul>
            
                        <div class="image-container">
                            <figure>
                                <img src="figure/post_token.jpeg" alt="Post-token augmentation transaction data">
                                <figcaption>Figure: Transaction data after token augmentation. The <code>memo</code> field now includes additional tokens: <code>&lt;D_day&gt;</code> for the day of the transaction, <code>&lt;M_month&gt;</code> for the month, and <code>&lt;W_D&gt;</code> for whole dollar amounts.</figcaption>
                            </figure>
                        </div>
                    </details>
            
                    <!-- Cleaning the Memo Fields -->
                    <details>
                        <summary>Cleaning the Memo Fields</summary>
                    
                        <p>To prepare the memo field for analysis, we first applied a series of cleaning steps to standardize the text. We transformed all text to lowercase for uniformity, removed any extraneous punctuation and symbols, and stripped out dates, state abbreviations, and recurring text that didn’t contribute to transaction categorization (e.g., “POS withdrawal”). Placeholder values such as multiple X’s were also removed.</p>
                    
                        <p>To prepare the dataset for modeling, we began by reviewing a sample of unique transaction memos from each category. This step provided insights into common patterns and inconsistencies, helping us determine the scope and focus of our cleaning tasks.</p>
                    
                        <div class="image-container">
                            <figure>
                                <img src="figure/nonclean_df.png" alt="Uncleaned Memo Dataframe">
                                <figcaption>Figure: This table shows the memo dataframe before cleaning. Variations in text format, inconsistent capitalization, and extraneous characters can be observed, which motivated the cleaning process to improve uniformity in transaction descriptions.</figcaption>
                            </figure>
            
                            <figure>
                                <img src="figure/clean_df.png" alt="Cleaned Memo Dataframe">
                                <figcaption>Figure: This table represents the cleaned memo dataframe, where we applied text preprocessing to ensure consistency across transaction descriptions. By converting to lowercase, removing punctuation, and standardizing certain tokens, we prepared the data for more accurate feature extraction and analysis.</figcaption>
                            </figure>
                        </div>
                    
                        <p>We then applied several transformations to standardize the data and improve its interpretability:</p>
                    
                        <ul>
                            <li><strong>Date Removal:</strong> We used regular expressions (RegEx) to locate and remove dates across entries in the memo column, as they did not contribute to the model's predictive goals and added unnecessary complexity. Placeholder patterns such as "XXXX" were also removed.</li>
            
                            <li><strong>Pattern Recognition:</strong> Specific patterns and keywords were identified as indicators of certain transaction categories. For example, "TST" was reliably associated with "Food and Beverages," while "APPLE.COM/BILL" was linked to "General Merchandise." These patterns were flagged to automate future classifications and reduce manual intervention.</li>
            
                            <li><strong>Selective Character Retention:</strong> To preserve potentially valuable information, certain characters such as dots ('.') were retained to keep URLs or email addresses intact within the memo field, which could provide clues to transaction categories.</li>
            
                            <li><strong>Transaction Labels:</strong> We identified recurring phrases such as "POS Withdrawal," location-specific markers (e.g., “CA 10/27” for state and date), and labels indicating recurring payments. These were removed as they did not contribute to the model’s predictive goals and added unnecessary complexity.</li>
            
                            <li><strong>Text Normalization:</strong> All text was converted to lowercase to reduce variability due to case differences.</li>
                        </ul>
                    </details>
                </section>


                <section id="models-results">
                    <h4><strong>Models and Results</strong></h4>

                    <!-- Logistic Regression Section -->
                    <details>
                        <summary><strong>Logistic Regression with TF-IDF Vectorization</strong></summary>
                        <p>
                            To classify transaction categories based on the <code>memo</code> text field, we first used a Logistic Regression model with TF-IDF vectorization. 
                            This method converts raw text data into numerical features, allowing the model to utilize term frequency patterns for classification.
                        </p>
                
                        <h4>TF-IDF Vectorization</h4>
                        <p>We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=5000:</strong> Limited to the top 5,000 most important terms.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Ignored terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Logistic Regression Model</h4>
                        <p>For classification, we configured the Logistic Regression model with:</p>
                        <ul>
                            <li><strong>solver='saga':</strong> Efficient for large datasets and supports L2 regularization.</li>
                            <li><strong>max_iter=200:</strong> Ensured convergence.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The model achieved 96.15% accuracy on the test set.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/log_reg_confusion.png" alt="Logistic Regression Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Logistic Regression Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Random Forest Section -->
                    <details>
                        <summary><strong>Random Forest with TF-IDF Vectorization</strong></summary>
                        <p>
                            To improve classification performance, we implemented a Random Forest model. 
                            While this model introduces greater complexity, its performance was slightly lower than Logistic Regression.
                        </p>
                
                        <h4>TF-IDF Vectorization</h4>
                        <p>We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=2000:</strong> Limited the number of features to 2,000.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Excluded terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Random Forest Model</h4>
                        <p>For classification, we configured the Random Forest model with:</p>
                        <ul>
                            <li><strong>n_estimators=100:</strong> Used 100 decision trees.</li>
                            <li><strong>max_depth=60:</strong> Restricted tree depth to prevent overfitting.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The Random Forest model achieved 84.28% accuracy on the test set.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/random_forest_confusion.png" alt="Random Forest Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Random Forest Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- FastText Section -->
                    <details>
                        <summary><strong>FastText for Text Classification</strong></summary>
                        <p>
                            FastText is a lightweight and efficient text classification model developed by Facebook AI. 
                            We trained this model to categorize transaction memos with high speed and accuracy.
                        </p>
                
                        <h4>Model Training</h4>
                        <p>The FastText model was trained using the following hyperparameters:</p>
                        <ul>
                            <li><strong>Epochs:</strong> 25</li>
                            <li><strong>Learning Rate:</strong> 1.0</li>
                            <li><strong>Word N-Grams:</strong> 2</li>
                            <li><strong>Embedding Dimension:</strong> 50</li>
                            <li><strong>Bucket Size:</strong> 200,000</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The FastText model achieved an impressive 98.93% accuracy.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/fasttext_confusion.png" alt="FastText Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for FastText Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Transformer LLM Section -->
                    <details>
                        <summary><strong>LLM with Transformer Model</strong></summary>
                        <p>
                            We implemented a Transformer-based model using <code>distilbert-base-uncased</code> to leverage deep learning for text classification. 
                            This model was trained to classify transaction memos with contextual embeddings.
                        </p>
                
                        <h4>Training Configuration</h4>
                        <p>The model was configured with the following hyperparameters:</p>
                        <ul>
                            <li><strong>Model Type:</strong> DistilBERT</li>
                            <li><strong>Maximum Length:</strong> 128 tokens</li>
                            <li><strong>Batch Size:</strong> 16</li>
                            <li><strong>Learning Rate:</strong> 2e-5</li>
                            <li><strong>Optimizer:</strong> AdamW</li>
                            <li><strong>Scheduler:</strong> Linear schedule with a warmup ratio of 0.1</li>
                            <li><strong>Gradient Clipping:</strong> Maximum value of 1.0</li>
                            <li><strong>Epochs:</strong> 3</li>
                        </ul>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/transformer_confusion.png" alt="Transformer Model Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Transformer Model.</figcaption>
                        </figure>
                    </details>
                </section>
            </section>

            <!-- Cash Score Section -->
            <section id="cash-score">
                <h3>Cash Score</h3>
            
                <!-- Methodology -->
                <section id="methodology">
                    <h4><strong>Methodology</strong></h4>
            
                    <!-- Exploratory Data Analysis -->
                    <details>
                        <summary>Exploratory Data Analysis</summary>
                        <p>We conducted an exploratory data analysis (EDA) to understand patterns in consumer spending, overdrafts, and transaction distributions. This helped us identify meaningful features for predicting credit risk.</p>
                    </details>
            
                    <!-- Balance Trends for Delinquent vs. Non-Delinquent Consumers -->
                    <details>
                        <summary>Comparing Delinquent and Non-Delinquent Consumers</summary>
                        <p>We analyzed balance trends over time to observe how cash flow differs between individuals who are delinquent and those who are not. Key findings include...</p>
                        <figure>
                            <img src="figure/balance_trends.png" alt="Balance Trends">
                            <figcaption>Figure: Average balance trends for delinquent vs. non-delinquent consumers.</figcaption>
                        </figure>
                    </details>
            
                    <!-- Feature Engineering -->
                    <details>
                        <summary>Feature Engineering</summary>
                        <p>We engineered a set of financial features that capture income stability, spending behavior, overdraft frequency, and transaction patterns.</p>
                        <ul>
                            <li><strong>Overdraft Frequency:</strong> The number of times a consumer overdrafted in the past 6 months.</li>
                            <li><strong>Spending Volatility:</strong> Standard deviation of monthly expenditures.</li>
                            <li><strong>Recurring Payments:</strong> Identifying transactions like rent, utilities, and subscriptions.</li>
                        </ul>
                    </details>
                </section>
            
                <!-- Models and Results -->
                <section id="models-results">
                    <h4><strong>Models and Results</strong></h4>
            
                    <!-- Model Training -->
                    <details>
                        <summary>Model Training</summary>
                        <p>We trained multiple models, including Logistic Regression, Random Forest, and Gradient Boosting, to predict delinquency risk.</p>
                        <ul>
                            <li><strong>Train-Test Split:</strong> 80% training, 20% testing.</li>
                            <li><strong>Feature Scaling:</strong> Normalization was applied to numerical features.</li>
                            <li><strong>Cross-Validation:</strong> We used k-fold cross-validation for model robustness.</li>
                        </ul>
                    </details>
            
                    <!-- Model Evaluation -->
                    <details>
                        <summary>Model Evaluation</summary>
                        <p>We evaluated model performance using precision, recall, F1-score, and AUC-ROC.</p>
                        <figure>
                            <img src="figure/model_evaluation.png" alt="Model Evaluation Metrics">
                            <figcaption>Figure: Model performance evaluation using key metrics.</figcaption>
                        </figure>
                    </details>
            
                    <!-- Feature Performance -->
                    <details>
                        <summary>Feature Performance</summary>
                        <p>We analyzed the most influential features for predicting delinquency. Overdraft frequency and spending patterns were among the top indicators.</p>
                        <figure>
                            <img src="figure/feature_importance.png" alt="Feature Importance">
                            <figcaption>Figure: Top predictive features ranked by importance.</figcaption>
                        </figure>
                    </details>
            
                    <!-- Model Performance -->
                    <details>
                        <summary>Model Performance</summary>
                        <p>We compared various models based on accuracy, F1-score, and inference time. Gradient Boosting provided the best tradeoff between performance and efficiency.</p>
                        <table>
                            <tr>
                                <th>Model</th>
                                <th>Accuracy</th>
                                <th>F1-Score</th>
                                <th>AUC-ROC</th>
                            </tr>
                            <tr>
                                <td>Logistic Regression</td>
                                <td>85.4%</td>
                                <td>0.79</td>
                                <td>0.88</td>
                            </tr>
                            <tr>
                                <td>Random Forest</td>
                                <td>89.1%</td>
                                <td>0.82</td>
                                <td>0.91</td>
                            </tr>
                            <tr>
                                <td>Gradient Boosting</td>
                                <td>92.3%</td>
                                <td>0.87</td>
                                <td>0.94</td>
                            </tr>
                        </table>
                    </details>
                </section>
            </section>

            <section id="results">
                <h3>Results</h3>
                <p>Our models achieved accuracy above 80%, with XGBoost providing the highest performance in predicting credit risk. The most influential features included overdraft frequency, balance volatility, and transaction categories related to high-risk spending.</p>
            </section>
            <section id="model-output">
                <h3>Model Output</h3>
                <p>The model assigns a probability of default score to each user, with key contributing factors highlighted through explainability techniques.</p>
            </section>
            <section id="discussion">
                <h3>Discussion</h3>
                <p>Our model demonstrates the potential for alternative credit scoring methods but faces challenges such as data bias and class imbalance. Future work will focus on refining the fairness and interpretability of the Cash Score.</p>
            </section>
            <section id="contribution">
                <h3>Contributions</h3>
                <p>Our project aims to create a fairer credit assessment system while maintaining accuracy and transparency. The Cash Score model reduces reliance on traditional credit history and promotes financial inclusivity.</p>
            </section>
            <a href="#" aria-label="Go to top of the page" id="top-btn">Go To Top &uarr;</a>
        </main>
        <footer>
            <div class="footer-container">
                <h3 id="contact">Contacts: </h3>
                <div class="contact">
                    <div class="contact-links">
                        <p>Jevan Chahal</p>
                        <a href="mailto:j2chahal@ucsd.edu"><i>j2chahal@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Hillary Chang</p>
                        <a href="mailto:hic001@ucsd.edu"><i>hic001@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kurumi Kaneko</p>
                        <a href="mailto:kskaneko@ucsd.edu"><i>kskaneko@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kevin Wong</p>
                        <a href="mailto:kew024@ucsd.edu"><i>kew024@ucsd.edu</i></a>
                    </div>
                </div>
                <p>This page was generated by <a
                        href="https://pages.github.com/" target="_blank">GitHub
                        Pages</a></p>
                <p>&copy; 2024 Jevan, Hillary, Kurumi, and Kevin. All
                    rights reserved.</p>
            </div>
        </footer>
    </body>
</html>
