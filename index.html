<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Cash Score: Alternative Credit Scoring</title>
        <meta name="description"
            content="2024 UC San Diego Data Science Capstone Project, written by Section B18 Group X.">
        <meta name="author" content="Jevan Chahal, Hillary Chang, Kurumi Kaneko, Kevin Wong">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Tilt+Warp&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:ital,wght@0,500;1,500&display=swap" rel="stylesheet">
        <link type="text/css" href="style.css" rel="stylesheet">
        <link href="media/ucsd-icon.png" rel="icon">
    </head>
    <body>
        <header>
            <h1>Cash Score: Alternative Credit Scoring</h1>
            <h2>Using Banking Transaction Data for Fairer Credit Assessment</h2>
            <div class="authors">
                <p>By:
                    <a href="mailto:j2chahal@ucsd.edu">Jevan Chahal</a>,
                    <a href="mailto:hic001@ucsd.edu">Hillary Chang</a>,
                    <a href="mailto:kskaneko@ucsd.edu">Kurumi Kaneko</a>, and
                    <a href="mailto:kew024@ucsd.edu">Kevin Wong</a>.
                </p>
                <p>Mentors:
                    <a href="mailto:brian.duke@prismdata.com">Brian Duke (Prism Data, Inc.)</a> and
                    <a href="mailto:kyle.nero@prismdata.com">Kyle Nero (Prism Data, Inc.)</a>.
                </p>
            </div>
            <div class="links">
                <a href="https://github.com/hillarychang/dsc180b-capstone-q2"
                    class="resource-links" target="_blank" rel="noopener noreferrer">View Code</a>
                <a href="document/poster.pdf" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Poster</a>
                <a href="document/report.pdf" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Report</a>
            </div>
            <nav>
                <ul>
                    <li><a href="#abstract">Abstract</a></li>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#methods">Methods</a></li>
                    <li><a href="#categorizing-transactions">Categorizing Transactions</a></li>
                    <li><a href="#cash-score">Cash Score</a></li>
                    <li><a href="#discussion">Discussion</a></li>
                    <li><a href="#contribution">Contributions</a></li>
                    <li><a href="#contact">Contacts</a></li>
                </ul>
            </nav>
        </header>
        <hr />
        <main>
            <section id="abstract">
                <h3>Abstract</h3>
                <p>The process of determining a creditor's trustworthiness is crucial within the context of bank data, given the ethical and regulatory constraints surrounding its use. Despite the vast quantity of available data, only a limited number of features are explicitly useful for machine learning applications, raising the question of how best to assess a customer's financial reliability. Our methodology involves refining bank data into meaningful categories using Natural Language Processing, estimating individual income based on transaction data alone, and evaluating creditworthiness with both accuracy and efficiency.</p>
            </section>
            
            <section id="introduction">
                <h3>Introduction</h3>
                <p>Determining a customer’s creditworthiness is essential for banks, influencing decisions about loans, credit cards, and other financial services. While traditional credit scores are widely used, they often fail to capture the full financial picture, especially for individuals with little to no credit history. Our project aims to bridge this gap by leveraging transaction data to assess financial behavior rather than relying solely on past credit activity. By analyzing spending habits, income consistency, and transaction types, we seek to create a scoring model that provides a more accurate and equitable reflection of financial reliability. As part of our project, we have implemented a data pipeline to support this model, including a categorization system that classifies transaction memos from vendors with high accuracy and low latency—ensuring that entries like "Amazon.com" are correctly categorized under "General Merchandise." This categorization plays a key role in our broader goal of developing the Cash Score, a numerical value ranging from 1 to 999 that predicts a consumer’s likelihood of defaulting on debt or credit. By equipping financial institutions with this tool, we aim to enhance risk assessment while ensuring that credit access is extended to those with strong banking histories, making lending practices more inclusive and effective.</p>
            </section>
            
            <section id="methods">
                <h3>Methods</h3>
                <p>Our methodology consists of feature engineering from transaction data, model training using various machine learning algorithms, and performance evaluation based on key metrics. The figure below shows our overall methodology:</p>
    
                <figure>
                    <img src="figure/methodology.png" alt="Overall Methodology Diagram">
                    <figcaption>Figure: Overview of our methodology.</figcaption>
                </figure>
            </section>

<!--        <p>Our project is structured around two primary objectives: categorizing transactions based on memos and developing the <strong>Cash Score</strong>, a predictive model for assessing a consumer’s likelihood of defaulting on credit.</p> -->

            <!-- Categorizing Transactions based on Memos -->
            <section id="categorizing-transactions">
                <h3>Categorizing Transactions Based on Memos</h3>

                <!-- Feature Creation and Selection -->
                <section id="feature-creation">
                    <h4 class="feature-title">Feature Creation and Selection</h4>
                
                    <!-- Token Augmentation -->
                    <details class="expandable-section">
                        <summary>Token Augmentation</summary>
                
                        <p>
                            To prepare the transaction data for further analysis and modeling, we enhanced the <code>memo</code> field by adding specific tokens based on transaction characteristics. 
                            This token augmentation adds additional context about transaction amounts and dates, helping the model learn more meaningful patterns.
                        </p>
                
                        <div class="image-container">
                            <figure>
                                <img src="figure/pre_token.jpeg" alt="Pre-token augmentation transaction data">
                                <figcaption>Figure: This is the transaction data cleaned but not token augmented yet. The <code>memo</code> field contains raw text descriptions without any added tokens.</figcaption>
                            </figure>
                        </div>
                
                        <p class="process-intro">The following steps outline the transformation process:</p>
                
                        <ul>
                            <li>
                                <strong>Whole Dollar Amount Identification:</strong> We added a token <code>&lt;W_D&gt;</code> to the <code>memo</code> field for transactions with whole dollar amounts.
                                This token helps identify transactions that might involve cash withdrawals or ATM transactions, as these are typically in round amounts (e.g., $20, $50).
                            </li>
                
                            <li>
                                <strong>Day of Transaction:</strong> For each transaction's posting date, we generated a day token in the format <code>&lt;D_day&gt;</code>, where <code>day</code> represents the specific 
                                day of the month. This token helps analyze patterns such as end-of-month or beginning-of-month spending habits.
                            </li>
                
                            <li>
                                <strong>Month of Transaction:</strong> Similarly, we generated a month token in the format <code>&lt;M_month&gt;</code> (where <code>month</code> is the numerical month) 
                                to capture seasonal or monthly spending trends.
                            </li>
                
                            <li>
                                <strong>Token Augmentation Process:</strong> For each row in the dataset, these tokens were concatenated to the <code>memo</code> field using the following transformation:
                                <p class="math-equation">memo = memo + whole_dollar_amount_token + day_token + month_token</p>
                            </li>
                        </ul>
                
                        <div class="image-container">
                            <figure>
                                <img src="figure/post_token.jpeg" alt="Post-token augmentation transaction data">
                                <figcaption>Figure: Transaction data after token augmentation. The <code>memo</code> field now includes additional tokens: <code>&lt;D_day&gt;</code> for the day of the transaction, <code>&lt;M_month&gt;</code> for the month, and <code>&lt;W_D&gt;</code> for whole dollar amounts.</figcaption>
                            </figure>
                        </div>
                    </details>
                </section>


                
<!--                 <section id="feature-creation">

                    <h4><strong>Feature Creation and Selection</strong></h4>
            
                    <!-- Token Augmentation -->
                    <details>
                        <summary>Token Augmentation</summary>
                    
                        <p>
                            To prepare the transaction data for further analysis and modeling, we enhanced the <code>memo</code> field by adding specific tokens based on transaction characteristics. 
                            This token augmentation adds additional context about transaction amounts and dates, helping the model learn more meaningful patterns.
                        </p>
                    
                        <div class="image-container">
                            <figure>
                                <img src="figure/pre_token.jpeg" alt="Pre-token augmentation transaction data">
                                <figcaption>Figure: This is the transaction data cleaned but not token augmented yet. The <code>memo</code> field contains raw text descriptions without any added tokens.</figcaption>
                            </figure>
                        </div>
                    
                        <p>The following steps outline the transformation process:</p>
                    
                        <ul>
                            <li>
                                <strong>Whole Dollar Amount Identification:</strong> We added a token <code>&lt;W_D&gt;</code> to the <code>memo</code> field for transactions with whole dollar amounts.
                                This token helps identify transactions that might involve cash withdrawals or ATM transactions, as these are typically in round amounts (e.g., $20, $50).
                            </li>
            
                            <li>
                                <strong>Day of Transaction:</strong> For each transaction's posting date, we generated a day token in the format <code>&lt;D_day&gt;</code>, where <code>day</code> represents the specific 
                                day of the month. This token helps analyze patterns such as end-of-month or beginning-of-month spending habits.
                            </li>
            
                            <li>
                                <strong>Month of Transaction:</strong> Similarly, we generated a month token in the format <code>&lt;M_month&gt;</code> (where <code>month</code> is the numerical month) 
                                to capture seasonal or monthly spending trends.
                            </li>
            
                            <li>
                                <strong>Token Augmentation Process:</strong> For each row in the dataset, these tokens were concatenated to the <code>memo</code> field using the following transformation:
<!--                                 <pre>
            memo = memo + whole_dollar_amount_token + day_token + month_token
                                </pre> -->
                                <p class="math-equation">memo = memo + whole_dollar_amount_token + day_token + month_token</p>
                            </li>
                        </ul>
            
                        <div class="image-container">
                            <figure>
                                <img src="figure/post_token.jpeg" alt="Post-token augmentation transaction data">
                                <figcaption>Figure: Transaction data after token augmentation. The <code>memo</code> field now includes additional tokens: <code>&lt;D_day&gt;</code> for the day of the transaction, <code>&lt;M_month&gt;</code> for the month, and <code>&lt;W_D&gt;</code> for whole dollar amounts.</figcaption>
                            </figure>
                        </div>
                    </details>
             -->
                    <!-- Cleaning the Memo Fields -->
                    <details>
                        <summary>Cleaning the Memo Fields</summary>
                    
                        <p>To prepare the memo field for analysis, we first applied a series of cleaning steps to standardize the text. We transformed all text to lowercase for uniformity, removed any extraneous punctuation and symbols, and stripped out dates, state abbreviations, and recurring text that didn’t contribute to transaction categorization (e.g., “POS withdrawal”). Placeholder values such as multiple X’s were also removed.</p>
                    
                        <p>To prepare the dataset for modeling, we began by reviewing a sample of unique transaction memos from each category. This step provided insights into common patterns and inconsistencies, helping us determine the scope and focus of our cleaning tasks.</p>
                    
                        <div class="image-container">
                            <figure>
                                <img src="figure/nonclean_df.png" alt="Uncleaned Memo Dataframe">
                                <figcaption>Figure: This table shows the memo dataframe before cleaning. Variations in text format, inconsistent capitalization, and extraneous characters can be observed, which motivated the cleaning process to improve uniformity in transaction descriptions.</figcaption>
                            </figure>
            
                            <figure>
                                <img src="figure/clean_df.png" alt="Cleaned Memo Dataframe">
                                <figcaption>Figure: This table represents the cleaned memo dataframe, where we applied text preprocessing to ensure consistency across transaction descriptions. By converting to lowercase, removing punctuation, and standardizing certain tokens, we prepared the data for more accurate feature extraction and analysis.</figcaption>
                            </figure>
                        </div>
                    
                        <p>We then applied several transformations to standardize the data and improve its interpretability:</p>
                    
                        <ul>
                            <li><strong>Date Removal:</strong> We used regular expressions (RegEx) to locate and remove dates across entries in the memo column, as they did not contribute to the model's predictive goals and added unnecessary complexity. Placeholder patterns such as "XXXX" were also removed.</li>
            
                            <li><strong>Pattern Recognition:</strong> Specific patterns and keywords were identified as indicators of certain transaction categories. For example, "TST" was reliably associated with "Food and Beverages," while "APPLE.COM/BILL" was linked to "General Merchandise." These patterns were flagged to automate future classifications and reduce manual intervention.</li>
            
                            <li><strong>Selective Character Retention:</strong> To preserve potentially valuable information, certain characters such as dots ('.') were retained to keep URLs or email addresses intact within the memo field, which could provide clues to transaction categories.</li>
            
                            <li><strong>Transaction Labels:</strong> We identified recurring phrases such as "POS Withdrawal," location-specific markers (e.g., “CA 10/27” for state and date), and labels indicating recurring payments. These were removed as they did not contribute to the model’s predictive goals and added unnecessary complexity.</li>
            
                            <li><strong>Text Normalization:</strong> All text was converted to lowercase to reduce variability due to case differences.</li>
                        </ul>
                    </details>
                </section>


                <section id="models-results">
                    <h4><strong>Models and Results</strong></h4>

                    <!-- Logistic Regression Section -->
                    <details>
                        <summary><strong>Logistic Regression with TF-IDF Vectorization</strong></summary>
                        <p>
                            To classify transaction categories based on the <code>memo</code> text field, we first used a Logistic Regression model with TF-IDF vectorization. 
                            This method converts raw text data into numerical features, allowing the model to utilize term frequency patterns for classification.
                        </p>
                
                        <h4>TF-IDF Vectorization</h4>
                        <p>We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=5000:</strong> Limited to the top 5,000 most important terms.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Ignored terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Logistic Regression Model</h4>
                        <p>For classification, we configured the Logistic Regression model with:</p>
                        <ul>
                            <li><strong>solver='saga':</strong> Efficient for large datasets and supports L2 regularization.</li>
                            <li><strong>max_iter=200:</strong> Ensured convergence.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The model achieved 96.15% accuracy on the test set.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/log_reg_confusion.png" alt="Logistic Regression Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Logistic Regression Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Random Forest Section -->
                    <details>
                        <summary><strong>Random Forest with TF-IDF Vectorization</strong></summary>
                        <p>
                            To improve classification performance, we implemented a Random Forest model. 
                            While this model introduces greater complexity, its performance was slightly lower than Logistic Regression.
                        </p>
                
                        <h4>TF-IDF Vectorization</h4>
                        <p>We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=2000:</strong> Limited the number of features to 2,000.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Excluded terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Random Forest Model</h4>
                        <p>For classification, we configured the Random Forest model with:</p>
                        <ul>
                            <li><strong>n_estimators=100:</strong> Used 100 decision trees.</li>
                            <li><strong>max_depth=60:</strong> Restricted tree depth to prevent overfitting.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The Random Forest model achieved 84.28% accuracy on the test set.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/random_forest_confusion.png" alt="Random Forest Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Random Forest Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- FastText Section -->
                    <details>
                        <summary><strong>FastText for Text Classification</strong></summary>
                        <p>
                            FastText is a lightweight and efficient text classification model developed by Facebook AI. 
                            We trained this model to categorize transaction memos with high speed and accuracy.
                        </p>
                
                        <h4>Model Training</h4>
                        <p>The FastText model was trained using the following hyperparameters:</p>
                        <ul>
                            <li><strong>Epochs:</strong> 25</li>
                            <li><strong>Learning Rate:</strong> 1.0</li>
                            <li><strong>Word N-Grams:</strong> 2</li>
                            <li><strong>Embedding Dimension:</strong> 50</li>
                            <li><strong>Bucket Size:</strong> 200,000</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The FastText model achieved an impressive 98.93% accuracy.</p>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/fasttext_confusion.png" alt="FastText Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for FastText Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Transformer LLM Section -->
                    <details>
                        <summary><strong>LLM with Transformer Model</strong></summary>
                        <p>
                            We implemented a Transformer-based model using <code>distilbert-base-uncased</code> to leverage deep learning for text classification. 
                            This model was trained to classify transaction memos with contextual embeddings.
                        </p>
                
                        <h4>Training Configuration</h4>
                        <p>The model was configured with the following hyperparameters:</p>
                        <ul>
                            <li><strong>Model Type:</strong> DistilBERT</li>
                            <li><strong>Maximum Length:</strong> 128 tokens</li>
                            <li><strong>Batch Size:</strong> 16</li>
                            <li><strong>Learning Rate:</strong> 2e-5</li>
                            <li><strong>Optimizer:</strong> AdamW</li>
                            <li><strong>Scheduler:</strong> Linear schedule with a warmup ratio of 0.1</li>
                            <li><strong>Gradient Clipping:</strong> Maximum value of 1.0</li>
                            <li><strong>Epochs:</strong> 3</li>
                        </ul>
                
                        <h4>Confusion Matrix</h4>
                        <figure>
                            <img src="figure/transformer_confusion.png" alt="Transformer Model Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Transformer Model.</figcaption>
                        </figure>
                    </details>
                </section>
            </section>

            <!-- Cash Score Section -->
            <section id="cash-score">
                <h3>Predicting Cash Score</h3>
            
                <!-- Methodology -->
                <section id="methodology">
                    <h4><strong>Methodology</strong></h4>
            
                    <!-- Exploratory Data Analysis -->
                    <details>
                        <summary>Exploratory Data Analysis</summary>
                        <p>We conducted an exploratory data analysis (EDA) to understand patterns in consumer spending, overdrafts, and transaction distributions. This helped us identify meaningful features for predicting credit risk.</p>
                    
                        <ul>
                            <li><strong>Transaction Patterns:</strong> Identified differences in transaction behaviors between delinquent and non-delinquent consumers, such as frequency and types of purchases.</li>
                            <li><strong>Seasonal Trends & Payday Effects:</strong> Examined how consumer spending fluctuates with seasonal trends, including holiday expenses and end-of-month payday spikes.</li>
                            <li><strong>Income Estimation:</strong> Estimated consumer income based on recurring transactions such as payroll deposits, rent, and utility payments.</li>
                            <li><strong>Impact of Fees & Overdrafts:</strong> Analyzed the effects of account fees, buy-now-pay-later (BNPL) transactions, and overdraft occurrences on overall financial stability.</li>
                        </ul>
                    </details>

            
                    <!-- Balance Trends for Delinquent vs. Non-Delinquent Consumers -->
                    <details>
                        <summary>Comparing Delinquent and Non-Delinquent Consumers</summary>
                        
                        <p>We analyzed balance trends over time to observe how cash flow differs between individuals who are delinquent and those who are not. Our key findings highlight financial stability differences based on account balance trends.</p>
                    
                        <ul>
                            <li><strong>Delinquent Consumers:</strong> Frequently experience negative balances, indicating financial instability and a higher risk of missing payments.</li>
                            <li><strong>Non-Delinquent Consumers:</strong> Maintain relatively stable balances with fewer occurrences of overdrafts, suggesting better financial health.</li>
                            <li><strong>Periodic Fluctuations:</strong> Balance trends exhibit cyclical patterns influenced by income deposits and spending habits.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/balance_trends.png" alt="Balance Trends">
                            <figcaption>Figure: Average balance trends for delinquent vs. non-delinquent consumers.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_delinquent.png" alt="Balance Trends for Delinquent Consumers">
                            <figcaption>Figure: Balance trends over time for five randomly selected delinquent consumers. The plot illustrates fluctuations and frequent occurrences of negative balances, highlighting financial instability.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_non_delinquent.png" alt="Balance Trends for Non-Delinquent Consumers">
                            <figcaption>Figure: Balance trends over time for five randomly selected non-delinquent consumers. Compared to delinquent consumers, these users maintain more stable balances with fewer instances of overdrafts.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_single_non_delinquent.png" alt="Balance for a Single Non-Delinquent Consumer">
                            <figcaption>Figure: Balance over time for a single non-delinquent consumer. The balance exhibits periodic fluctuations, potentially due to income deposits and spending patterns, but remains above zero during the observed period.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_single_delinquent.png" alt="Balance for a Single Delinquent Consumer">
                            <figcaption>Figure: Balance over time for a single delinquent consumer. This consumer frequently experiences negative balances, indicating financial distress and an increased risk of missing payments.</figcaption>
                        </figure>
                    </details>
            
                    <!-- Feature Engineering -->
                    <details>
                        <summary>Feature Engineering</summary>
                        
                        <p>We engineered multiple features relevant to the prediction of delinquency, focusing on balance trends, transaction behaviors, and account types. These features help in assessing financial stability and predicting default risk.</p>
                    
                        <ul>
                            <li><strong>Balance Features:</strong> Negative balance ratio, balance trends, payday effects.</li>
                            <li><strong>Transaction-Based Features:</strong> Credit vs. debit transaction volume, category-based spending breakdown.</li>
                            <li><strong>Temporal Features:</strong> Spending frequency over time, accounting for longevity effects.</li>
                            <li><strong>Account Types:</strong> Features based on the types of accounts a consumer has.</li>
                            <li><strong>Overdraft Frequency:</strong> The number of times a consumer overdrafted in the past 6 months.</li>
                            <li><strong>Spending Volatility:</strong> Standard deviation of monthly expenditures.</li>
                            <li><strong>Recurring Payments:</strong> Identifying transactions like rent, utilities, and subscriptions.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/spending_balance_ratio.png" alt="Spending Balance Ratio">
                            <figcaption>Figure: Spending balance ratio feature created to measure how much consumers spend relative to their balance. This helps assess financial stability and risk of delinquency.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/standardized_credit_balance.png" alt="Standardized Credit and Balance">
                            <figcaption>Figure: Feature engineering step where credit and balance were standardized to allow for easier model interpretability and comparisons across different financial profiles.</figcaption>
                        </figure>
                    
                    </details>
                </section>
            
                <!-- Models and Results -->
                <section id="models-results">
                    <h4><strong>Models and Results</strong></h4>
            
                    <!-- Model Training -->
                    <details>
                        <summary>Model Training</summary>
                        
                        <p>We trained multiple machine learning models to predict delinquency risk. These models range from simple interpretable baselines to advanced gradient boosting and neural networks, each chosen for its ability to capture different aspects of financial behavior.</p>
                    
                        <ul>
                            <li><strong>Logistic Regression:</strong> A simple, interpretable baseline model.</li>
                            <li><strong>Random Forest:</strong> Captures non-linear financial relationships.</li>
                            <li><strong>XGBoost:</strong> Optimized for structured financial data.</li>
                            <li><strong>Neural Networks:</strong> Captures complex spending patterns.</li>
                            <li><strong>LightGBM:</strong> Efficient with categorical features and handling imbalanced data.</li>
                            <li><strong>Balanced RF:</strong> Handles class imbalance by weighting classes or resampling.</li>
                            <li><strong>CatBoost:</strong> Handles categorical features automatically and is robust to overfitting.</li>
                            <li><strong>HistGB:</strong> A histogram-based gradient boosting model.</li>
                            <li><strong>RUSBoost:</strong> Combines Random Under-Sampling with boosting to address class imbalance while maintaining predictive performance.</li>
                        </ul>
                    
                        <p>Additionally, we applied best practices to optimize our model training process:</p>
                        <ul>
                            <li><strong>Train-Test Split:</strong> 80% training, 20% testing.</li>
                            <li><strong>Feature Scaling:</strong> Normalization was applied to numerical features.</li>
                            <li><strong>Cross-Validation:</strong> We used k-fold cross-validation to ensure model robustness and prevent overfitting.</li>
                        </ul>
                    </details>

                    <!-- Model Evaluation -->
                    <details>
                        <summary>Model Evaluation</summary>
                    
                        <p>We evaluated model performance using a combination of classification metrics, computation efficiency, and techniques to address class imbalance.</p>
                    
                        <h4>Key Evaluation Metrics</h4>
                        <ul>
                            <li><strong>Accuracy and F1-Score:</strong> Measures overall classification performance.</li>
                            <li><strong>ROC-AUC:</strong> Evaluates the model's ability to differentiate between delinquent and non-delinquent users.</li>
                            <li><strong>Precision and Recall:</strong> Precision measures correctly identified positive cases, while recall evaluates the proportion of actual delinquents correctly detected.</li>
                            <li><strong>Training Time:</strong> The time required to train the model, crucial for scalability in production.</li>
                            <li><strong>Prediction Time:</strong> The time taken to make predictions on new data.</li>
                            <li><strong>Feature Importance:</strong> Highlights the most predictive variables that contribute to delinquency classification.</li>
                        </ul>
                    
                        <h4>Addressing Class Imbalance</h4>
                        <p>Since delinquents make up only ~8.4% of the dataset, we applied techniques to balance the dataset and improve model robustness.</p>
                        <ul>
                            <li><strong>SMOTE & SMOTEENN:</strong> Synthetic Minority Over-sampling Technique (SMOTE) and SMOTE-Edited Nearest Neighbors (SMOTEENN) were used to generate synthetic samples and refine class distributions.</li>
                            <li><strong>Feature Normalization:</strong> Standardization of key numerical features ensured that different scales did not affect model performance.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/model_evaluation.png" alt="Model Evaluation Metrics">
                            <figcaption>Figure: Model performance evaluation using key metrics.</figcaption>
                        </figure>
                    </details>

            
                    <!-- Feature Performance -->
                    <details>
                        <summary>Feature Performance</summary>
                    
                        <p>We analyzed the most influential features for predicting delinquency using an <strong>XGBoost Classifier (XGBClassifier)</strong>. Overdraft frequency and spending patterns were among the top indicators.</p>
                    
                        <h4>Key Predictors of Delinquency</h4>
                        <ul>
                            <li><strong>Account Type - Savings:</strong> Indicates whether a consumer has a savings account.</li>
                            <li><strong>Account Fees:</strong> Total sum of all account-related fees.</li>
                            <li><strong>Credit Score:</strong> Credit score of a consumer, which serves as a baseline feature.</li>
                            <li><strong>Overdraft Median:</strong> Median amount of overdraft transactions.</li>
                            <li><strong>Account Fees Median:</strong> Median amount of account-related fees.</li>
                            <li><strong>BNPL Std:</strong> Standard deviation of Buy-Now-Pay-Later (BNPL) transactions.</li>
                            <li><strong>Overdraft Count:</strong> Total number of overdraft transactions.</li>
                            <li><strong>Investment Income Median:</strong> Median amount of investment income.</li>
                            <li><strong>Investment Income Count:</strong> Number of investment-related transactions.</li>
                            <li><strong>Banking Catch All Std:</strong> Standard deviation of a consumer’s transactions within this category.</li>
                        </ul>
                    
                        <h4>Top Features from XGBClassifier</h4>
                        <table>
                            <tr>
                                <th>Feature</th>
                                <th>Importance</th>
                                <th>Correlation</th>
                            </tr>
                            <tr>
                                <td>Account Type - Savings</td>
                                <td>0.044091</td>
                                <td>-0.099071</td>
                            </tr>
                            <tr>
                                <td>Account Fees Count</td>
                                <td>0.037083</td>
                                <td>0.020680</td>
                            </tr>
                            <tr>
                                <td>Credit Score</td>
                                <td>0.030284</td>
                                <td>-0.249976</td>
                            </tr>
                            <tr>
                                <td>Overdraft Median</td>
                                <td>0.026024</td>
                                <td>0.000407</td>
                            </tr>
                            <tr>
                                <td>Account Fees Median</td>
                                <td>0.021387</td>
                                <td>0.001497</td>
                            </tr>
                            <tr>
                                <td>BNPL Std</td>
                                <td>0.021323</td>
                                <td>0.034083</td>
                            </tr>
                            <tr>
                                <td>Overdraft Count</td>
                                <td>0.021319</td>
                                <td>0.066101</td>
                            </tr>
                            <tr>
                                <td>Investment Income Median</td>
                                <td>0.018767</td>
                                <td>0.004675</td>
                            </tr>
                            <tr>
                                <td>Investment Income Count</td>
                                <td>0.017630</td>
                                <td>-0.026354</td>
                            </tr>
                            <tr>
                                <td>Banking Catch All Std</td>
                                <td>0.014733</td>
                                <td>-0.010585</td>
                            </tr>
                        </table>
                    
                        <figure>
                            <img src="figure/feature_importance.png" alt="Feature Importance">
                            <figcaption>Figure: Top predictive features ranked by importance.</figcaption>
                        </figure>
                    </details>

                    <!-- Model Performance -->
                    <details>
                        <summary>Model Performance</summary>
                    
                        <p>We compared various models based on accuracy, F1-score, and inference time. The <strong>Histogram-Based Gradient Boosting (HistGB)</strong> model achieved the highest ROC-AUC score, while <strong>Balanced Random Forest (Balanced RF)</strong> provided the best accuracy. Logistic Regression was the fastest to train but had lower performance in comparison.</p>
                    
                        <h4>Model Performance Metrics</h4>
                        <table>
                            <tr>
                                <th>Model</th>
                                <th>ROC-AUC</th>
                                <th>Accuracy</th>
                                <th>Precision</th>
                                <th>Recall</th>
                                <th>F1-Score</th>
                            </tr>
                            <tr>
                                <td><strong>HistGB</strong></td>
                                <td><strong>0.8424</strong></td>
                                <td>0.9135</td>
                                <td>0.8898</td>
                                <td>0.9135</td>
                                <td>0.8995</td>
                            </tr>
                            <tr>
                                <td>XGBoost</td>
                                <td>0.8393</td>
                                <td>0.9101</td>
                                <td>0.8890</td>
                                <td>0.9101</td>
                                <td>0.8981</td>
                            </tr>
                            <tr>
                                <td>LightGBM</td>
                                <td>0.8289</td>
                                <td>0.9139</td>
                                <td>0.8894</td>
                                <td>0.9139</td>
                                <td>0.8994</td>
                            </tr>
                            <tr>
                                <td>CatBoost</td>
                                <td>0.8232</td>
                                <td>0.9158</td>
                                <td>0.8879</td>
                                <td>0.9158</td>
                                <td>0.8989</td>
                            </tr>
                            <tr>
                                <td>RUSBoost</td>
                                <td>0.8059</td>
                                <td>0.8263</td>
                                <td>0.9050</td>
                                <td>0.8263</td>
                                <td>0.8579</td>
                            </tr>
                            <tr>
                                <td>Random Forest</td>
                                <td>0.7943</td>
                                <td>0.9155</td>
                                <td>0.8853</td>
                                <td>0.9155</td>
                                <td>0.8973</td>
                            </tr>
                            <tr>
                                <td><strong>Balanced RF</strong></td>
                                <td>0.7916</td>
                                <td><strong>0.9197</strong></td>
                                <td>0.8926</td>
                                <td><strong>0.9197</strong></td>
                                <td><strong>0.9022</strong></td>
                            </tr>
                            <tr>
                                <td>Logistic Regression</td>
                                <td>0.7611</td>
                                <td>0.7594</td>
                                <td><strong>0.9067</strong></td>
                                <td>0.7594</td>
                                <td>0.8140</td>
                            </tr>
                        </table>
                    
                        <h4>Time-Based Model Performance</h4>
                        <table>
                            <tr>
                                <th>Model</th>
                                <th>Training Time (s)</th>
                                <th>Prediction Time (s)</th>
                            </tr>
                            <tr>
                                <td>HistGB</td>
                                <td>8.76</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>XGBoost</td>
                                <td>5.68</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>LightGBM</td>
                                <td>4.40</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>CatBoost</td>
                                <td>42.61</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>RUSBoost</td>
                                <td>23.32</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>Random Forest</td>
                                <td>22.37</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td>Balanced RF</td>
                                <td>27.23</td>
                                <td>TODO</td>
                            </tr>
                            <tr>
                                <td><strong>Logistic Regression</strong></td>
                                <td><strong>2.35</strong></td>
                                <td>TODO</td>
                            </tr>
                        </table>
                    
                        <h4>ROC-AUC Comparison</h4>
                        <p>The following figure compares the ROC-AUC scores of all models tested, demonstrating their ability to distinguish between delinquent and non-delinquent consumers.</p>
                        <figure>
                            <img src="figure/auc_roc_all_models.png" alt="AUC-ROC Scores">
                            <figcaption>Figure: Comparison of AUC-ROC scores for all models.</figcaption>
                        </figure>
                    
                    </details>
                        
                </section>
            </section>

            <section id="discussion">
                <h3>Discussion</h3>
                <p>Our model demonstrates the potential for alternative credit scoring methods but faces challenges such as data bias and class imbalance. Future work will focus on refining the fairness and interpretability of the Cash Score.</p>
            </section>
            <section id="contribution">
                <h3>Contributions</h3>
                <p>Our project aims to create a fairer credit assessment system while maintaining accuracy and transparency. The Cash Score model reduces reliance on traditional credit history and promotes financial inclusivity.</p>
            </section>
            <a href="#" aria-label="Go to top of the page" id="top-btn">Go To Top &uarr;</a>
        </main>
        <footer>
            <div class="footer-container">
                <h3 id="contact">Contacts: </h3>
                <div class="contact">
                    <div class="contact-links">
                        <p>Jevan Chahal</p>
                        <a href="mailto:j2chahal@ucsd.edu"><i>j2chahal@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Hillary Chang</p>
                        <a href="mailto:hic001@ucsd.edu"><i>hic001@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kurumi Kaneko</p>
                        <a href="mailto:kskaneko@ucsd.edu"><i>kskaneko@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kevin Wong</p>
                        <a href="mailto:kew024@ucsd.edu"><i>kew024@ucsd.edu</i></a>
                    </div>
                </div>
                <p>This page was generated by <a
                        href="https://pages.github.com/" target="_blank">GitHub
                        Pages</a></p>
                <p>&copy; 2024 Jevan, Hillary, Kurumi, and Kevin. All
                    rights reserved.</p>
            </div>
        </footer>
    </body>
</html>
