<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Cash Score: Alternative Credit Scoring</title>
        <meta name="description"
            content="2024 UC San Diego Data Science Capstone Project, written by Section B18 Group X.">
        <meta name="author" content="Jevan Chahal, Hillary Chang, Kurumi Kaneko, Kevin Wong">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Tilt+Warp&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Red+Hat+Display:ital,wght@0,500;1,500&display=swap" rel="stylesheet">
        <link type="text/css" href="style.css" rel="stylesheet">
        <link href="media/ucsd-icon.png" rel="icon">
    </head>
    <body>
        <header>
            <h1>Cash Score: Alternative Credit Scoring</h1>
            <h2>Using Banking Transaction Data for Fairer Credit Assessment</h2>
            <div class="authors">
                <p>By:
                    <a href="mailto:j2chahal@ucsd.edu">Jevan Chahal</a>,
                    <a href="mailto:hic001@ucsd.edu">Hillary Chang</a>,
                    <a href="mailto:kskaneko@ucsd.edu">Kurumi Kaneko</a>, and
                    <a href="mailto:kew024@ucsd.edu">Kevin Wong</a>.
                </p>
                <p>Mentors:
                    <a href="mailto:brian.duke@prismdata.com">Brian Duke (Prism Data, Inc.)</a>
                    <a href="mailto:kyle.nero@prismdata.com">Kyle Nero (Prism Data, Inc.)</a> and
                    <a href="mailto:berk.ustun@prismdata.com">Berk Ustun (Prism Data, Inc.)</a>.
                </p>
            </div>
            <div class="links">
                <a href="https://github.com/hillarychang/dsc180b-capstone-q2"
                    class="resource-links" target="_blank" rel="noopener noreferrer">View Code</a>
                <a href="https://github.com/hillarychang/dsc180b-capstone-q2/tree/main/poster" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Poster</a>
                <a href="https://github.com/hillarychang/dsc180b-capstone-q2/tree/main/report" class="resource-links" target="_blank" rel="noopener noreferrer">View
                    Report</a>
            </div>
            <nav>
                <ul>
                    <li><a href="#abstract">Abstract</a></li>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#methods">Methods</a></li>
                    <li><a href="#categorizing-transactions">Categorizing Transactions</a></li>
                    <li><a href="#cash-score">Cash Score</a></li>
                    <li><a href="#discussion">Discussion</a></li>
                    <li><a href="#contribution">Contributions</a></li>
                    <li><a href="#contact">Contacts</a></li>
                </ul>
            </nav>
        </header>
        <hr />
        <main>
            <section id="abstract">
                <h3>Abstract</h3>
                <p>The process of determining a creditor's trustworthiness is crucial within the context of bank data, given the ethical and regulatory constraints surrounding its use. Despite the vast quantity of available data, only a limited number of features are explicitly useful for machine learning applications, raising the question of how best to assess a customer's financial reliability. Our methodology involves refining bank data into meaningful categories using Natural Language Processing, estimating individual income based on transaction data alone, and evaluating creditworthiness with both accuracy and efficiency.</p>
            </section>
            
            <section id="introduction">
                <h3>Introduction</h3>
                <p>Determining a customer’s creditworthiness is essential for banks, influencing decisions about loans, credit cards, and other financial services. While traditional credit scores are widely used, they often fail to capture the full financial picture, especially for individuals with little to no credit history. Our project aims to bridge this gap by leveraging transaction data to assess financial behavior rather than relying solely on past credit activity. By analyzing spending habits, income consistency, and transaction types, we seek to create a scoring model that provides a more accurate and equitable reflection of financial reliability. As part of our project, we have implemented a data pipeline to support this model, including a categorization system that classifies transaction memos from vendors with high accuracy and low latency—ensuring that entries like "Amazon.com" are correctly categorized under "General Merchandise." This categorization plays a key role in our broader goal of developing the Cash Score, a numerical value ranging from 1 to 999 that predicts a consumer’s likelihood of defaulting on debt or credit. By equipping financial institutions with this tool, we aim to enhance risk assessment while ensuring that credit access is extended to those with strong banking histories, making lending practices more inclusive and effective.</p>
            </section>
            
            <section id="methods">
                <h3>Methods</h3>
                <p>Our methodology consists of feature engineering from transaction data, model training using various machine learning algorithms, and performance evaluation based on key metrics. The figure below shows our overall methodology:</p>
    
                <figure>
                    <img src="figure/methodology.png" alt="Overall Methodology Diagram">
                    <figcaption>Figure: Overview of our methodology.</figcaption>
                </figure>
            </section>

<!--        <p>Our project is structured around two primary objectives: categorizing transactions based on memos and developing the <strong>Cash Score</strong>, a predictive model for assessing a consumer’s likelihood of defaulting on credit.</p> -->

            <!-- Categorizing Transactions based on Memos -->
            <section id="categorizing-transactions">
                <h3>Categorizing Transactions Based on Memos</h3>

                <!-- Feature Creation and Selection -->
                <section id="feature-creation">
                    <h4 class="feature-title">Feature Creation and Selection</h4>
                
                    <!-- Token Augmentation -->
                    <details class="expandable-section">
                        <summary>Token Augmentation</summary>
                
                        <p>
                            To prepare the transaction data for further analysis and modeling, we enhanced the <code>memo</code> field by adding specific tokens based on transaction characteristics. 
                            This token augmentation adds additional context about transaction amounts and dates, helping the model learn more meaningful patterns.
                        </p>
                
                        <div class="image-container">
                            <figure>
                                <img src="figure/pre_token.png" alt="Pre-token augmentation transaction data">
                                <figcaption>Figure: This is the transaction data cleaned but not token augmented yet. The <code>memo</code> field contains raw text descriptions without any added tokens.</figcaption>
                            </figure>
                        </div>
                
                        <p class="process-intro">The following steps outline the transformation process:</p>
                
                        <ul>
                            <li>
                                <strong>Whole Dollar Amount Identification:</strong> We added a token <code>&lt;W_D&gt;</code> to the <code>memo</code> field for transactions with whole dollar amounts.
                                This token helps identify transactions that might involve cash withdrawals or ATM transactions, as these are typically in round amounts (e.g., $20, $50).
                            </li>
                
                            <li>
                                <strong>Day of Transaction:</strong> For each transaction's posting date, we generated a day token in the format <code>&lt;D_day&gt;</code>, where <code>day</code> represents the specific 
                                day of the month. This token helps analyze patterns such as end-of-month or beginning-of-month spending habits.
                            </li>
                
                            <li>
                                <strong>Month of Transaction:</strong> Similarly, we generated a month token in the format <code>&lt;M_month&gt;</code> (where <code>month</code> is the numerical month) 
                                to capture seasonal or monthly spending trends.
                            </li>
                
                            <li>
                                <strong>Token Augmentation Process:</strong> For each row in the dataset, these tokens were concatenated to the <code>memo</code> field using the following transformation:
                                <p class="math-equation">memo = memo + whole_dollar_amount_token + day_token + month_token</p>
                            </li>
                        </ul>
                
                        <div class="image-container">
                            <figure>
                                <img src="figure/post_token.png" alt="Post-token augmentation transaction data">
                                <figcaption>Figure: Transaction data after token augmentation. The <code>memo</code> field now includes additional tokens: <code>&lt;D_day&gt;</code> for the day of the transaction, <code>&lt;M_month&gt;</code> for the month, and <code>&lt;W_D&gt;</code> for whole dollar amounts.</figcaption>
                            </figure>
                        </div>
                    </details>


                    <details class="expandable-section">
                    <summary>Cleaning the Memo Fields</summary>
                
                    <p>
                        To prepare the memo field for analysis, we first applied a series of cleaning steps to standardize the text. 
                        We transformed all text to lowercase for uniformity, removed any extraneous punctuation and symbols, 
                        and stripped out dates, state abbreviations, and recurring text that didn’t contribute to transaction categorization 
                        (e.g., “POS withdrawal”). Placeholder values such as multiple X’s were also removed.
                    </p>
                
                    <p>
                        To prepare the dataset for modeling, we began by reviewing a sample of unique transaction memos from each category. 
                        This step provided insights into common patterns and inconsistencies, helping us determine the scope and focus of our cleaning tasks.
                    </p>
                
                    <div class="image-container">
                        <figure>
                            <img src="figure/nonclean_df.png" alt="Uncleaned Memo Dataframe">
                            <figcaption>
                                Figure: This table shows the memo dataframe before cleaning. Variations in text format, inconsistent capitalization, and extraneous characters 
                                can be observed, which motivated the cleaning process to improve uniformity in transaction descriptions.
                            </figcaption>
                        </figure>
                
                        <figure>
                            <img src="figure/clean_df.png" alt="Cleaned Memo Dataframe">
                            <figcaption>
                                Figure: This table represents the cleaned memo dataframe, where we applied text preprocessing to ensure consistency across transaction descriptions. 
                                By converting to lowercase, removing punctuation, and standardizing certain tokens, we prepared the data for more accurate feature extraction and analysis.
                            </figcaption>
                        </figure>
                    </div>
                
                    <p class="transformation-intro">
                        We then applied several transformations to standardize the data and improve its interpretability:
                    </p>
                
                    <ul>
                        <li class="cleaning-step">
                            <strong>Date Removal:</strong> We used regular expressions (RegEx) to locate and remove dates across entries in the memo column, 
                            as they did not contribute to the model's predictive goals and added unnecessary complexity. Placeholder patterns such as "XXXX" were also removed.
                        </li>
                
                        <li class="cleaning-step">
                            <strong>Pattern Recognition:</strong> Specific patterns and keywords were identified as indicators of certain transaction categories. 
                            For example, "TST" was reliably associated with "Food and Beverages," while "APPLE.COM/BILL" was linked to "General Merchandise." 
                            These patterns were flagged to automate future classifications and reduce manual intervention.
                        </li>
                
                        <li class="cleaning-step">
                            <strong>Selective Character Retention:</strong> To preserve potentially valuable information, certain characters such as dots ('.') were retained 
                            to keep URLs or email addresses intact within the memo field, which could provide clues to transaction categories.
                        </li>
                
                        <li class="cleaning-step">
                            <strong>Transaction Labels:</strong> We identified recurring phrases such as "POS Withdrawal," location-specific markers 
                            (e.g., “CA 10/27” for state and date), and labels indicating recurring payments. These were removed as they did not contribute 
                            to the model’s predictive goals and added unnecessary complexity.
                        </li>
                
                        <li class="cleaning-step">
                            <strong>Text Normalization:</strong> All text was converted to lowercase to reduce variability due to case differences.
                        </li>
                    </ul>
                </details>
                </section>

                <!-- Models and Results -->
                <section id="models-results">
                    <h4 class="feature-title"><strong>Models and Results</strong></h4>

                    <!-- Logistic Regression Section -->
                    <details class="expandable-section">
                        <summary><strong>Logistic Regression with TF-IDF Vectorization</strong></summary>
                        <p>
                            To classify transaction categories based on the <code>memo</code> text field, we first used a Logistic Regression model with TF-IDF vectorization. 
                            This method converts raw text data into numerical features, allowing the model to utilize term frequency patterns for classification.
                        </p>
                
                        <h4 class="process-intro">TF-IDF Vectorization</h4>
                        <p class="process-intro">We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=5000:</strong> Limited to the top 5,000 most important terms.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Ignored terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Logistic Regression Model</h4>
                        <p class="process-intro">For classification, we configured the Logistic Regression model with:</p>
                        <ul>
                            <li><strong>solver='saga':</strong> Efficient for large datasets and supports L2 regularization.</li>
                            <li><strong>max_iter=200:</strong> Ensured convergence.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The model achieved 96.15% accuracy on the test set.</p>
                
                        <h4 class="process-intro">Confusion Matrix</h4>
                        <figure>
                            <img src="figure/log_reg_confusion.png" alt="Logistic Regression Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Logistic Regression Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Random Forest Section -->
                    <details class="expandable-section">
                        <summary><strong>Random Forest with TF-IDF Vectorization</strong></summary>
                        <p>
                            To improve classification performance, we implemented a Random Forest model. 
                            While this model introduces greater complexity, its performance was slightly lower than Logistic Regression.
                        </p>
                
                        <h4 class="process-intro">TF-IDF Vectorization</h4>
                        <p class="process-intro">We employed a TF-IDF vectorizer to transform text data into a matrix of features with the following parameters:</p>
                        <ul>
                            <li><strong>max_features=2000:</strong> Limited the number of features to 2,000.</li>
                            <li><strong>max_df=0.95:</strong> Ignored terms appearing in more than 95% of documents.</li>
                            <li><strong>min_df=5:</strong> Excluded terms appearing in fewer than 5 documents.</li>
                        </ul>
                
                        <h4>Random Forest Model</h4>
                        <p class="process-intro">For classification, we configured the Random Forest model with:</p>
                        <ul>
                            <li><strong>n_estimators=100:</strong> Used 100 decision trees.</li>
                            <li><strong>max_depth=60:</strong> Restricted tree depth to prevent overfitting.</li>
                            <li><strong>n_jobs=-1:</strong> Utilized all available CPU cores for parallel training.</li>
                        </ul>
                
                        <p><strong>Accuracy:</strong> The Random Forest model achieved 84.28% accuracy on the test set.</p>
                
                        <h4 class="process-intro">Confusion Matrix</h4>
                        <figure>
                            <img src="figure/random_forest_confusion.png" alt="Random Forest Confusion Matrix">
                            <figcaption>Figure: Confusion Matrix for Random Forest Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- FastText Section -->
                    <details class="expandable-section">
                        <summary><strong>FastText for Text Classification</strong></summary>
                        <p>
                            FastText is a lightweight and efficient text classification model developed by Facebook AI. 
                            We trained this model to categorize transaction memos with high speed and accuracy.
                        </p>
                    
                        <h4 class="process-intro">Model Training</h4>
                        <p class="process-intro">The FastText model was trained using the following hyperparameters:</p>
                        <ul>
                            <li><strong>Epochs:</strong> 25</li>
                            <li><strong>Learning Rate:</strong> 1.0</li>
                            <li><strong>Word N-Grams:</strong> 2</li>
                            <li><strong>Embedding Dimension:</strong> 50</li>
                            <li><strong>Bucket Size:</strong> 200,000</li>
                        </ul>
                    
                        <p class="process-intro"><strong>Accuracy:</strong> The FastText model achieved an impressive 98.9% accuracy.</p>
                    
                        <h4 class="process-intro">Classification Report</h4>
                        <p class="process-intro">The classification report below presents precision, recall, and F1-scores for different transaction categories.</p>
                    
                        <figure>
                            <img src="figure/fasttext_classification_report.png" alt="FastText Classification Report">
                            <figcaption>Figure: Classification Report for FastText Model.</figcaption>
                        </figure>
                    </details>
                
                    <!-- Transformer LLM Section -->
                    <details class="expandable-section">
                        <summary><strong>LLM with Transformer Model</strong></summary>
                        <p>
                            We implemented a Transformer-based model using <code>distilbert-base-uncased</code> to leverage deep learning for text classification. 
                            This model was trained to classify transaction memos with contextual embeddings.
                        </p>
                    
                        <h4 class="process-intro">Training Configuration</h4>
                        <p class="process-intro">The model was configured with the following hyperparameters:</p>
                        <ul>
                            <li><strong>Model Type:</strong> DistilBERT</li>
                            <li><strong>Maximum Length:</strong> 128 tokens</li>
                            <li><strong>Batch Size:</strong> 16</li>
                            <li><strong>Learning Rate:</strong> 2e-5</li>
                            <li><strong>Optimizer:</strong> AdamW</li>
                            <li><strong>Scheduler:</strong> Linear schedule with a warmup ratio of 0.1</li>
                            <li><strong>Gradient Clipping:</strong> Maximum value of 1.0</li>
                            <li><strong>Epochs:</strong> 3</li>
                        </ul>
                    
                        <h4 class="process-intro">Model Performance</h4>
                        <p class="process-intro">
                            The Transformer model was evaluated on 9 transaction categories. After one epoch, it achieved:
                        </p>
                        <ul>
                            <li><strong>Training Loss:</strong> 0.1361</li>
                            <li><strong>Validation Loss:</strong> 0.0603</li>
                            <li><strong>Validation Accuracy:</strong> 98.56%</li>
                            <li><strong>Evaluation Time:</strong> 2 minutes and 43 seconds</li>
                        </ul>
                    
                        <p>
                            While the Transformer model achieved high accuracy, it exhibited higher latency compared to simpler models, 
                            requiring longer processing times for both training and evaluation.
                        </p>
                    </details>

                </section>
            </section>

            <!-- Cash Score Section -->
            <section id="cash-score">
                <h3>Predicting Cash Score</h3>
            
                <!-- Methodology -->
                <section id="methodology">
                    <h4 class="feature-title"><strong>Methodology</strong></h4>
            
                    <!-- Exploratory Data Analysis -->
                    <details class="expandable-section">
                        <summary>Exploratory Data Analysis</summary>
                        <p>We conducted an exploratory data analysis (EDA) to understand patterns in consumer spending, overdrafts, and transaction distributions. This helped us identify meaningful features for predicting credit risk.</p>
                    
                        <ul>
                            <li><strong>Transaction Patterns:</strong> Identified differences in transaction behaviors between delinquent and non-delinquent consumers, such as frequency and types of purchases.</li>
                            <li><strong>Seasonal Trends & Payday Effects:</strong> Examined how consumer spending fluctuates with seasonal trends, including holiday expenses and end-of-month payday spikes.</li>
                            <li><strong>Income Estimation:</strong> Estimated consumer income based on recurring transactions such as payroll deposits, rent, and utility payments.</li>
                            <li><strong>Impact of Fees & Overdrafts:</strong> Analyzed the effects of account fees, buy-now-pay-later (BNPL) transactions, and overdraft occurrences on overall financial stability.</li>
                        </ul>
                    </details>

            
                    <!-- Balance Trends for Delinquent vs. Non-Delinquent Consumers -->
                    <details class="expandable-section">
                        <summary>Comparing Delinquent and Non-Delinquent Consumers</summary>
                        
                        <p>We analyzed balance trends over time to observe how cash flow differs between individuals who are delinquent and those who are not. Our key findings highlight financial stability differences based on account balance trends.</p>
                    
                        <ul>
                            <li><strong>Delinquent Consumers:</strong> Frequently experience negative balances, indicating financial instability and a higher risk of missing payments.</li>
                            <li><strong>Non-Delinquent Consumers:</strong> Maintain relatively stable balances with fewer occurrences of overdrafts, suggesting better financial health.</li>
                            <li><strong>Periodic Fluctuations:</strong> Balance trends exhibit cyclical patterns influenced by income deposits and spending habits.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/balance_trends.png" alt="Balance Trends">
                            <figcaption>Figure: Average balance trends for delinquent vs. non-delinquent consumers.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_delinquent.png" alt="Balance Trends for Delinquent Consumers">
                            <figcaption>Figure: Balance trends over time for five randomly selected delinquent consumers. The plot illustrates fluctuations and frequent occurrences of negative balances, highlighting financial instability.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_non_delinquent.png" alt="Balance Trends for Non-Delinquent Consumers">
                            <figcaption>Figure: Balance trends over time for five randomly selected non-delinquent consumers. Compared to delinquent consumers, these users maintain more stable balances with fewer instances of overdrafts.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_single_non_delinquent.png" alt="Balance for a Single Non-Delinquent Consumer">
                            <figcaption>Figure: Balance over time for a single non-delinquent consumer. The balance exhibits periodic fluctuations, potentially due to income deposits and spending patterns, but remains above zero during the observed period.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/balance_single_delinquent.png" alt="Balance for a Single Delinquent Consumer">
                            <figcaption>Figure: Balance over time for a single delinquent consumer. This consumer frequently experiences negative balances, indicating financial distress and an increased risk of missing payments.</figcaption>
                        </figure>
                    </details>
            
                    <!-- Feature Engineering -->
                    <details class="expandable-section">
                        <summary>Feature Engineering</summary>
                    
                        <p>We engineered multiple features relevant to the prediction of delinquency, focusing on balance trends, transaction behaviors, and account types. These features help in assessing financial stability and predicting default risk.</p>
                    
                        <ul>
                            <li><strong>Balance Features:</strong> Negative balance ratio, balance trends, payday effects.</li>
                            <li><strong>Transaction-Based Features:</strong> Credit vs. debit transaction volume, category-based spending breakdown.</li>
                            <li><strong>Temporal Features:</strong> Spending frequency over time, accounting for longevity effects.</li>
                            <li><strong>Account Types:</strong> Features based on the types of accounts a consumer has.</li>
                            <li><strong>Overdraft Frequency:</strong> The number of times a consumer overdrafted in the past 6 months.</li>
                            <li><strong>Spending Volatility:</strong> Standard deviation of monthly expenditures.</li>
                            <li><strong>Recurring Payments:</strong> Identifying transactions like rent, utilities, and subscriptions.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/spending_balance_ratio.png" alt="Spending Balance Ratio">
                            <figcaption>Figure: Spending balance ratio feature created to measure how much consumers spend relative to their balance. This helps assess financial stability and risk of delinquency.</figcaption>
                        </figure>
                    
                        <figure>
                            <img src="figure/standardized_credit_balance.png" alt="Standardized Credit and Balance">
                            <figcaption>Figure: Feature engineering step where credit and balance were standardized to allow for easier model interpretability and comparisons across different financial profiles.</figcaption>
                        </figure>
                    
                        <!-- Container for side-by-side plots -->
                        <div style="display: flex; justify-content: space-between; gap: 10px;">
                            <figure style="flex: 1; text-align: center;">
                                <img src="figure/shap_values2.png" alt="Feature Importance using Shap Values" style="max-width: 100%; height: auto;">
                                <figcaption>Figure: Feature Importance using Shap Values.</figcaption>
                            </figure>
                    
                            <figure style="flex: 1; text-align: center;">
                                <img src="figure/feature_plot_credit_score.png" alt="Delinquency Percentage vs Credit Score" style="max-width: 100%; height: auto;">
                                <figcaption>Figure: Delinquency Percentage vs Credit Score.</figcaption>
                            </figure>
                        </div>
                    
                    </details>
                </section>
            
                <!-- Models and Results -->
                <section id="models-results">
                    <h4 class="feature-title"><strong>Models and Results</strong></h4>                    
                    <!-- Model Training -->
                    <details class="expandable-section">
                        <summary>Model Training</summary>
                        
                        <p>We trained multiple machine learning models to predict delinquency risk. These models range from simple interpretable baselines to advanced gradient boosting and ensemble methods, each chosen for its ability to capture different aspects of financial behavior.</p>
                        
                        <ul>
                            <li><strong>Logistic Regression:</strong> A simple, interpretable baseline model.</li>
                            <li><strong>Random Forest:</strong> Captures non-linear financial relationships.</li>
                            <li><strong>XGBoost:</strong> Optimized for structured financial data.</li>
                            <li><strong>LightGBM:</strong> Efficient with categorical features and handling imbalanced data.</li>
                            <li><strong>Balanced RF:</strong> Handles class imbalance by weighting classes or resampling.</li>
                            <li><strong>CatBoost:</strong> Handles categorical features automatically and is robust to overfitting.</li>
                            <li><strong>HistGB:</strong> A histogram-based gradient boosting model.</li>
                            <li><strong>RUSBoost:</strong> Combines Random Under-Sampling with boosting to address class imbalance while maintaining predictive performance.</li>
                        </ul>
                    </details>
                    
                    <div style="margin-bottom: 20px;"></div>

                    <!-- Model Evaluation -->
                    <details class="expandable-section">
                        <summary>Model Evaluation</summary>
                        <p>We evaluated model performance using key classification metrics:</p>
                        <ul>
                            <li><strong>Accuracy and F1-Score:</strong> Measures overall classification performance.</li>
                            <li><strong>ROC-AUC:</strong> Evaluates the model's ability to differentiate between delinquent and non-delinquent users.</li>
                            <li><strong>Precision and Recall:</strong> Measures positive case identification accuracy.</li>
                            <li><strong>Training Time:</strong> Measures how long each model takes to train.</li>
                            <li><strong>Prediction Time:</strong> Measures the time taken to make predictions on new data.</li>
                        </ul>

                        <p>To mitigate the class imbalance (delinquents only 8.4% of dataset), we used:</p>
                        <ul>
                            <li><strong>SMOTE & SMOTEENN:</strong> Oversampling techniques.</li>
                            <li><strong>Feature Normalization:</strong> Standardization of key variables.</li>
                        </ul>
                    </details>
                    
                    <!-- Feature Importance for Different Models -->
                    <details class="expandable-section">
                        <summary>Feature Importance</summary>
                        <p>Top predictive features for different models:</p>
                        <figure>
                            <img src="figure/feature_importance_rf.png" alt="Random Forest Feature Importance">
                            <figcaption>Figure: Feature importance from Random Forest.</figcaption>
                        </figure>
                        <figure>
                            <img src="figure/feature_importance_xgb.png" alt="XGBoost Feature Importance">
                            <figcaption>Figure: Feature importance from XGBoost.</figcaption>
                        </figure>
                        <figure>
                            <img src="figure/feature_importance_brf.png" alt="Balanced Random Forest Feature Importance">
                            <figcaption>Figure: Feature importance from Balanced Random Forest</figcaption>
                        </figure>
                    </details>
                    
                    <details class="expandable-section">                        
                        <h4>Confusion Matrices</h4>
                        <div class="confusion-matrix-grid">
                            <figure>
                                <img src="figure/conf_matrix_logreg.png" alt="Logistic Regression Confusion Matrix">
                                <figcaption>Figure: Logistic Regression Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_rf.png" alt="Random Forest Confusion Matrix">
                                <figcaption>Figure: Random Forest Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_lightgbm.png" alt="LightGBM Confusion Matrix">
                                <figcaption>Figure: LightGBM Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_balanced_rf.png" alt="Balanced RF Confusion Matrix">
                                <figcaption>Figure: Balanced Random Forest Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_xgb.png" alt="XGBoost Confusion Matrix">
                                <figcaption>Figure: XGBoost Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_catboost.png" alt="CatBoost Confusion Matrix">
                                <figcaption>Figure: CatBoost Confusion Matrix.</figcaption>
                            </figure>
                            <figure>
                                <img src="figure/conf_matrix_rusboost.png" alt="RUSBoost Confusion Matrix">
                                <figcaption>Figure: RUSBoost Confusion Matrix.</figcaption>
                            </figure>
                        </div>
                
                        <summary>Model Performance</summary>
                        <table class="center-table">
                            <tr>
                                <th>Model</th>
                                <th>ROC-AUC</th>
                                <th>Accuracy</th>
                                <th>Precision</th>
                                <th>Recall</th>
                                <th>F1-Score</th>
                                <th>Train Time (s)</th>
                                <th>Predict Time (s)</th>
                            </tr>
                            <tr>
                                <td>LightGBM</td>
                                <td>0.832</td>
                                <td>0.914</td>
                                <td>0.889</td>
                                <td>0.914</td>
                                <td>0.899</td>
                                <td>2.208</td>
                                <td>0.000018</td>
                            </tr>
                            <tr>
                                <td>HistGB</td>
                                <td>0.823</td>
                                <td>0.913</td>
                                <td>0.892</td>
                                <td>0.913</td>
                                <td>0.901</td>
                                <td>3.196</td>
                                <td>0.000021</td>
                            </tr>
                            <tr>
                                <td>CatBoost</td>
                                <td>0.821</td>
                                <td>0.917</td>
                                <td>0.893</td>
                                <td>0.917</td>
                                <td>0.902</td>
                                <td>14.511</td>
                                <td>0.000005</td>
                            </tr>
                            <tr>
                                <td>XGBoost</td>
                                <td>0.818</td>
                                <td>0.914</td>
                                <td>0.893</td>
                                <td>0.914</td>
                                <td>0.902</td>
                                <td>1.635</td>
                                <td>0.000006</td>
                            </tr>
                            <tr>
                                <td>RUSBoost</td>
                                <td>0.802</td>
                                <td>0.829</td>
                                <td>0.903</td>
                                <td>0.829</td>
                                <td>0.859</td>
                                <td>6.954</td>
                                <td>0.000012</td>
                            </tr>
                            <tr>
                                <td>Balanced RF</td>
                                <td>0.800</td>
                                <td>0.914</td>
                                <td>0.890</td>
                                <td>0.914</td>
                                <td>0.900</td>
                                <td>13.670</td>
                                <td>0.000034</td>
                            </tr>
                            <tr>
                                <td>Random Forest</td>
                                <td>0.800</td>
                                <td>0.915</td>
                                <td>0.893</td>
                                <td>0.915</td>
                                <td>0.902</td>
                                <td>14.364</td>
                                <td>0.000088</td>
                            </tr>
                            <tr>
                                <td>Logistic Regression</td>
                                <td>0.777</td>
                                <td>0.743</td>
                                <td>0.911</td>
                                <td>0.743</td>
                                <td>0.803</td>
                                <td>0.348</td>
                                <td>0.000003</td>
                            </tr>
                        </table>
                    </details>
                    
                    <details class="expandable-section">
                        <summary>Conclusion</summary>
                    
                        <p>Our analysis highlights the effectiveness of different machine learning models in predicting delinquency risk. Among them, LightGBM demonstrated the highest ROC-AUC score, making it the most effective model for our use case.</p>
                    
                        <ul>
                            <li><strong>LightGBM:</strong> Highest ROC-AUC score, best overall performance.</li>
                            <li><strong>Reason Score:</strong> Features that most contributed to risk classification.</li>
                            <li><strong>Cash Score:</strong> Distribution of calculated scores for financial assessment.</li>
                        </ul>
                    
                        <figure>
                            <img src="figure/auc_roc_all_models.png" alt="AUC-ROC Scores">
                            <figcaption>Figure: ROC-AUC curve comparison for all models.</figcaption>
                        </figure>
                    
                        <!-- Container for side-by-side plots -->
                        <div style="display: flex; justify-content: space-between; gap: 10px;">
                            <figure style="flex: 1; text-align: center;">
                                <img src="figure/top_reasons.png" alt="Distribution of Common Reasons" style="max-width: 100%; height: auto;">
                                <figcaption>Figure: Distribution of Common Reasons.</figcaption>
                            </figure>
                    
                            <figure style="flex: 1; text-align: center;">
                                <img src="figure/cash_score_dist2.png" alt="Distribution of Cash Scores" style="max-width: 100%; height: auto;">
                                <figcaption>Figure: Distribution of Cash Scores.</figcaption>
                            </figure>
                        </div>
                    </details>
                </section>

            </section>

            <section id="discussion">
                <h3>Discussion</h3>
                <p>Our model demonstrates the potential for alternative credit scoring methods but faces challenges such as data bias and class imbalance. Future work will focus on refining the fairness and interpretability of the Cash Score.</p>
            </section>
            <section id="contribution">
                <h3>Contributions</h3>
                <p>Our project aims to create a fairer credit assessment system while maintaining accuracy and transparency. The Cash Score model reduces reliance on traditional credit history and promotes financial inclusivity.</p>
            </section>
            <a href="#" aria-label="Go to top of the page" id="top-btn">Go To Top &uarr;</a>
        </main>
        <footer>
            <div class="footer-container">
                <h3 id="contact">Contacts: </h3>
                <div class="contact">
                    <div class="contact-links">
                        <p>Jevan Chahal</p>
                        <a href="mailto:j2chahal@ucsd.edu"><i>j2chahal@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Hillary Chang</p>
                        <a href="mailto:hic001@ucsd.edu"><i>hic001@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kurumi Kaneko</p>
                        <a href="mailto:kskaneko@ucsd.edu"><i>kskaneko@ucsd.edu</i></a>
                    </div>
                    <div class="contact-links">
                        <p>Kevin Wong</p>
                        <a href="mailto:kew024@ucsd.edu"><i>kew024@ucsd.edu</i></a>
                    </div>
                </div>
                <p>&copy; 2025 Jevan, Hillary, Kurumi, and Kevin. All
                    rights reserved.</p>
            </div>
        </footer>
    </body>
</html>
